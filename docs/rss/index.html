<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[The Seeker]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>The Seeker</title><link>http://localhost:2368/</link></image><generator>Ghost 4.25</generator><lastBuildDate>Sun, 12 Dec 2021 07:11:54 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Object Detection, Tracking and Suspicious Activity Recognition for
Maritime Surveillance using Thermal Vision]]></title><description><![CDATA[<p></p><h2 id="abstract">Abstract</h2><p>In a world of a globalized economy, maritime surveillance is a crucial element. Today maritime transportation is considered to carry more than 90% of long-distance world trade. Due to this rapid growth in marine traffic, security and safety have arisen as key issues. Along with that, real-time detection of</p>]]></description><link>http://localhost:2368/object-detection-tracking-and-suspicious-activity-recognition-formaritime-surveillance-using-thermal-vision/</link><guid isPermaLink="false">61b592e720f42f030fa09034</guid><category><![CDATA[Projects]]></category><dc:creator><![CDATA[Sakuna Jayasundara]]></dc:creator><pubDate>Sun, 12 Dec 2021 07:10:14 GMT</pubDate><media:content url="http://localhost:2368/content/images/2021/12/PTI02_10_2021_000230B_1613623895951_1613623935527_1638100241965.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://localhost:2368/content/images/2021/12/PTI02_10_2021_000230B_1613623895951_1613623935527_1638100241965.jpg" alt="Object Detection, Tracking and Suspicious Activity Recognition for
Maritime Surveillance using Thermal Vision"><p></p><h2 id="abstract">Abstract</h2><p>In a world of a globalized economy, maritime surveillance is a crucial element. Today maritime transportation is considered to carry more than 90% of long-distance world trade. Due to this rapid growth in marine traffic, security and safety have arisen as key issues. Along with that, real-time detection of maritime activities has become essential to monitor and control fishing activities, smuggling, human trafficking, and maritime pollution. Sri Lanka is a country where most coastal families depend on a daily wage incurred by fishing and the safety of these fishermen is crucial not only to themselves but also to their families. Additionally, during the recent past, Sri Lankan Navy has seized many large consignments of drugs during a short period of time within its maritime borders.</p><p>With this project, we propose a system that is capable not only to detect objects within the surveillance area but also to detect a set of pre-identified suspicious activities happening within the borders. We believe this will be an ideal replacement to the current system available which is to manually detect both objects and classify activities as suspicious or not. With the detection of any such suspicious activities, the system is capable of alerting the relevant authorities in real-time which makes it superior to the available traditional method with an additional benefit of increased safety of security personnel. One key objective of this project is to be able to detect both objects and activities happening at any time of the day. Hence, thermal imagery is used for the development of the models and for real-time detection.</p><p>Many of the currently available systems are limited to object detection in marine<br>environments using RGB imagery while activity detection and object tracking<br>as maritime surveillance is not a very common area of research. With this project, we propose a novel deep learning solution that is capable of object detection, activity detection, tracking, and early identification of suspicious activities using thermal images in maritime environments.</p><p>The final solution will run on inference hardware where the video feed from the thermal camera will be fed into our proposed system which is capable of carrying out the above-mentioned tasks and visualize the results on the user interface developed.</p><h2 id="methodology">Methodology</h2><h3 id="system-architecture">System Architecture</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-11.53.10.png" class="kg-image" alt="Object Detection, Tracking and Suspicious Activity Recognition for
Maritime Surveillance using Thermal Vision" loading="lazy" width="1392" height="666" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-11.53.10.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-11.53.10.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-11.53.10.png 1392w" sizes="(min-width: 720px) 720px"><figcaption>System Architecture</figcaption></figure><p>The proposed system consists of the following components: </p><ul><li>The Thermal Camera that captures a thermal feed.</li><li>On each of the frames fed by the thermal camera, the object detection algorithm runs and detects objects and sends the information of the object locations (bound-ing box coordinates) to the object tracking algorithm.</li><li>The object tracking algorithm keeps a track of all objects in the scene.</li><li>The tracked objects and the temporal information are sent to the activity detectionalgorithm to detect whether any actions are taking place and if so whether theyare suspicious or not.</li><li>Outputs of each of these algorithms are sent to the user interface to be recorded and monitored.</li></ul><h3 id="thermal-camera">Thermal Camera</h3><p><a href="https://www.flir.com">FLIR</a> M232 thermal camera that is specifically designed for marine applications is selected to capture a thermal video feed. &#xA0;Some key specifications are given in the table below. &#xA0;We use the real time streaming protocol (RTSP) to stream thermal video from the camera to the user interface.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.00.50.png" class="kg-image" alt="Object Detection, Tracking and Suspicious Activity Recognition for
Maritime Surveillance using Thermal Vision" loading="lazy" width="1348" height="1224" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-12.00.50.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-12.00.50.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.00.50.png 1348w" sizes="(min-width: 720px) 720px"><figcaption>Camera Specifications</figcaption></figure><h3 id="datasets">Datasets</h3><p>For the key target deliverables in our work, we have used publicly available datasets to train and check the accuracy of our own algorithms and to compare the effectiveness ofeach different work type.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.06.38.png" class="kg-image" alt="Object Detection, Tracking and Suspicious Activity Recognition for
Maritime Surveillance using Thermal Vision" loading="lazy" width="1210" height="824" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-12.06.38.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-12.06.38.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.06.38.png 1210w" sizes="(min-width: 720px) 720px"><figcaption>Datasets for Maritime Object Detection</figcaption></figure><h3 id="object-detection">Object Detection</h3><p>Our system first requires detecting objects in the maritime environment that are laterused by the object tracking algorithm to set the heuristic bounding boxes to commencetracking. &#xA0;Deep Learning based object detection &#xA0;has recently become highlypopular owing to its high accuracy and high inference speed compared to contemporaryimage processing methods. With a vast variety of algorithms to choose from, there area few that can be shortlisted as having provided state-of-the-art results over the pastyears. These alternatives are compared in the below table using the reproduced results on the MS COCO benchmark dataset.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.11.01.png" class="kg-image" alt="Object Detection, Tracking and Suspicious Activity Recognition for
Maritime Surveillance using Thermal Vision" loading="lazy" width="1040" height="474" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-12.11.01.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-12.11.01.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.11.01.png 1040w" sizes="(min-width: 720px) 720px"><figcaption>Algorithm comparison for Object Detection</figcaption></figure><p>Based on the above evaluation and some comprehensive research, we chose <a href="https://github.com/xingyizhou/CenterNet">CenterNet</a> as our object detection framework. </p><h3 id="object-tracking">Object Tracking</h3><p>MOT algorithms address a more complex problem oftracking multiple objects in a dynamic environment preserving the accuracy and thespeed. &#xA0;Even though there are several multi-object trackers available, a tracker with alow complexity and a high accuracy would be the most suitable solution for an onlineand real-time task. Hence we considered the following trackers mentioned in the below table. The results were obtained on the <a href="https://motchallenge.net">MOT benchmark dataset</a>. Based on these results, the <a href="https://arxiv.org/abs/1602.00763">Simple Online Real-time Tracker (SORT)</a> demonstrates the best performance under real-time constraints with competitive tracking accuracy measured by MOTA and MOTP. Thus we adopted SORT to develop our final pipeline that is capable of detectingand tracking multiple marine objects from an input video sequence.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.15.41.png" class="kg-image" alt="Object Detection, Tracking and Suspicious Activity Recognition for
Maritime Surveillance using Thermal Vision" loading="lazy" width="954" height="424" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-12.15.41.png 600w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.15.41.png 954w" sizes="(min-width: 720px) 720px"><figcaption>Evaluation results of Multi-Object Trackers</figcaption></figure><p><strong>Merging Object Detection and Tracking</strong></p><p>Our complete Object Detection + Tracking pipeline can be shown as follows.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.18.31.png" class="kg-image" alt="Object Detection, Tracking and Suspicious Activity Recognition for
Maritime Surveillance using Thermal Vision" loading="lazy" width="1272" height="698" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-12.18.31.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-12.18.31.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.18.31.png 1272w" sizes="(min-width: 720px) 720px"><figcaption>Complete Object Detection and Tracking Pipeline</figcaption></figure><h3 id="activity-detection">Activity Detection</h3><p>The most critical and challenging component of our system is detecting and localizingthe actions that occur in the video stream we obtain through the thermal camera bothin spatial and temporal dimensions. &#xA0;In this section, &#xA0;a detailed analysis of the action detection algorithms is presented together with the novel architecture we designed forgeneral activity detection using the inspiration obtained from the analysis.</p><p>After doing some research we have found following activity detection frameworks.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.25.02.png" class="kg-image" alt="Object Detection, Tracking and Suspicious Activity Recognition for
Maritime Surveillance using Thermal Vision" loading="lazy" width="1238" height="672" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-12.25.02.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-12.25.02.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.25.02.png 1238w" sizes="(min-width: 720px) 720px"><figcaption>Alternative Activity Detection Frameworks</figcaption></figure><p>Due to many problems and performance issues in the existing frameworks, we decided to create our own novel activity detection architecture which led us to start a research. </p><p><strong>Novel Activity Detection Pipeline</strong></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.28.03.png" class="kg-image" alt="Object Detection, Tracking and Suspicious Activity Recognition for
Maritime Surveillance using Thermal Vision" loading="lazy" width="1250" height="412" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-12.28.03.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-12.28.03.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.28.03.png 1250w" sizes="(min-width: 720px) 720px"><figcaption>Activity Detection Architecture</figcaption></figure><p>If you want to know more about our research, checkout this <a href="https://sakunah.xyz/korsal/index.html">article</a>.</p><h2 id="user-interface">User Interface</h2><p>As the final step of our project, we developed a user-friendly interface shown in below figure to showcase tracked objects and detected activities.We chose PyQt5, the python binding of Qt which is heavily used in software develop-ment and runs on platforms supported by Qt which includes Windows, Linux, macOS,and Android. &#xA0;PyQt is licensed under the GPL v3 and the Riverbank Commercial license.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.32.02.png" class="kg-image" alt="Object Detection, Tracking and Suspicious Activity Recognition for
Maritime Surveillance using Thermal Vision" loading="lazy" width="1238" height="760" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-12.32.02.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-12.32.02.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-12.32.02.png 1238w" sizes="(min-width: 720px) 720px"><figcaption>User Interface</figcaption></figure><h2 id="conclusion">Conclusion</h2><p>This project, and the research work done as part of it, provide significant evidence to show that an automated surveillance system for maritime security using thermal images at both day and night is possible. The project goes on to develop the application as far as is possible given the limited data available for training and testing the developed algorithms. Additionally, the research work carried out achieves significant performance gains over existing real-time activity detection algorithms, extending the boundaries of deep learning techniques in the domain of activity detection. The completion of this system in its totality requires access to, and annotation of large volumes of potentially sensitive data. Obtaining and annotating this data will be a time-consuming and challenging task. However, given the success demonstrated so far, we believe that this system will have significant benefits for the Navy of Sri Lanka, and in general for the whole country.</p><h2 id="resources">Resources</h2><p>If you want checkout the code and the project report for a clear understanding visit my <a href="https://github.com/sakunaharinda/FYP-Maritime_Surveillance">github repository</a> and there you will find all the references to mentioned algorithms and frameworks.</p><p>So that&apos;s it !! I&apos;m hoping to write another article about deep-learning based GUI designing using Python in the future. </p>]]></content:encoded></item><item><title><![CDATA[FPGA Based Processor Implementation]]></title><description><![CDATA[<p>In this post, I am going to talk about one of my main and very interesting project done in the university. As the title suggests, it is FPGA based processor implementation done to downsample an image. So let&apos;s dive in.</p><h2 id="introduction">Introduction</h2><p>The goal of this project is to</p>]]></description><link>http://localhost:2368/fpga-based-processor-implementation/</link><guid isPermaLink="false">61b4ec71f714c603360260eb</guid><category><![CDATA[Projects]]></category><dc:creator><![CDATA[Sakuna Jayasundara]]></dc:creator><pubDate>Sat, 11 Dec 2021 18:46:12 GMT</pubDate><media:content url="http://localhost:2368/content/images/2021/12/photo-1597862624292-45748390b00e.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://localhost:2368/content/images/2021/12/photo-1597862624292-45748390b00e.jpg" alt="FPGA Based Processor Implementation"><p>In this post, I am going to talk about one of my main and very interesting project done in the university. As the title suggests, it is FPGA based processor implementation done to downsample an image. So let&apos;s dive in.</p><h2 id="introduction">Introduction</h2><p>The goal of this project is to design a microprocessor for a specified task and the Central Processing Unit (CPU) structure, while simulating it using a Hardware Description Language (HDL) such as Verilog and to implement the given task using a programmable logic device, preferably a Field Programmable Gate Arrays (FPGA). This report contains documents regarding the structure of the microprocessor and CPU design, the Verilog codes used to configure it and finally the physical configuration of the hardware.</p><ul><li><strong>Central Processing Unit</strong><br>Central Processing Unit (CPU) widely considered as the brain of any computer device, is the place where all the instructions are carried out by performing logical operations, arithmetic operations, control operations and input/output operations. Hence, as explained above, CPU contains of two main components, the processor and the control unit while having a memory component and I/O components connecting to the CPU separately.</li><li><strong>Microprocessor </strong><br>Microprocessor can be considered as the most important part when it comes to a CPU. It is responsible for processing a unique set of instructions and processes. Its design is responsible to carry out logical and arithmetic operations. Microprocessors consists of many integrated circuits which holds thousands of components such as transistors etc.<br><br>Microprocessors are very important figures when it comes to modern computer designs and applications. With the high usage of microprocessors in the current market, various types of task- oriented microprocessors are available today. In this project we plan to design and implement a specific task-oriented microprocessor with maximum optimization.<br></li></ul><p>Project contains 3 main parts. </p><ul><li><strong>Sending the serial encoded pixel values to the FPGA and storing process</strong><br>Image is transmitted to the FPGA using UART (Universal Asynchronous Receiver/Transmitter) with 50MHz clock frequency. Image is sent by byte by byte serially using 4 states. They are START, IDLE, DATA, STOP. We built a python kernel using several libraries such as pyserial to communicate with RS232 port of the FPGA via the serial cable. After the image is sent to the FPGA we are notified by a LED. Then the process is triggered by a flag which is sent from the data memory module we built.<br><br>Since there were some errors when it comes to saving bits and storing them, we chose not to use the UART module and use the in-memory module for the data transfer process.<br><br>After the process is finished state of the processor is changed to the transmit state from process state. Then the UART transmitter is activated and it sends the processed down sampled image back to the computer for comparing and displaying purposes.<br><br>We can use any size of image in the architecture we&#x2019;re proposing, and pixel values will then be saved into the primary memory in the processor architecture. For the calculations and processing tasks in the latter sections, the values saved in these memory locations will be used.</li><li><strong>Down-sampling process</strong><br>With the values stored in the primary memory of the CPU, we begin our processing part by filtering the image using a gaussian kernel. The filtered image values will be saved in the remaining memory slots of the main memory of the CPU. Then we begin our down-sampling process and according to the instructions that have been pre-defined by us, the processor will carry them out and finally save the down-sampled image in the main memory, overlapping the original image at the first place. The down-sampling process will be given in order to the microprocessor by us according to an Instruction Set Architecture (ISA) that is being developed uniquely to our microprocessor. Each instruction in the ISA will contain few micro instructions and they will be fetched accordingly form the Instruction Register according to the algorithm we&#x2019;ve developed. The ISA and each registers task will be discussed later in this report.</li><li><strong>Transmitting the result back to the user</strong><br>The down sampled image will be sent back to the user in a similar manner used in transmitting the image. </li></ul><h2 id="instruction-set-architecture">Instruction Set Architecture </h2><p>As explained above, the main task of this processor is to down-sample any given image by a factor of two. In this section, we&#x2019;ll look at the architecture we have implemented to acquire this task and the use of all the special case and general case registers.</p><ul><li><strong>PC - </strong>A 10-bit Program Counter which keeps the main memory address of the next instruction to be fetched.</li><li><strong>IR - </strong>A 32-bit Instruction Register which holds the Instructions fetched.</li><li><strong>MAR - </strong>A 32-bit register</li><li><strong>AC - </strong>A 32-bit Accumulator which saves the intermediate values after a &#xA0; computation and the pixels values to be written/ read from data memory</li><li><strong>SOR - </strong>A 32-bit register which points the source location of the pixel</li><li><strong>DSTR - </strong>A 32-bit register which points the destination location of the pixel.</li><li><strong>COUN - </strong>A 32-bit register which is used for incrementing purposes.</li><li><strong>R1 - </strong>A 32-bit register used for general purpose and to store constant data </li><li><strong>R2 - </strong>A 32-bit register used for general purpose and to store constant data</li><li><strong>R3 - </strong>A 32-bit register used for general purpose and to store constant data</li></ul><h3 id="datapath">Datapath<br></h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.22.07.png" class="kg-image" alt="FPGA Based Processor Implementation" loading="lazy" width="902" height="1112" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-00.22.07.png 600w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.22.07.png 902w" sizes="(min-width: 720px) 720px"><figcaption>Datapath of the Processor</figcaption></figure><h3 id="process-cycle">Process Cycle</h3><p>The processing of the written algorithm happens in 3 stages which are fetch, decode and execute. In this section we&#x2019;ll take a brief look at these 3 stages and the operation of all these stages.</p><ul><li><strong>Fetch Instructions</strong><br>FETCH cycle consists of 2 stages with FETCH1 being set as the next state at the beginning.<br>FETCH1: Instruction Read <br>FETCH2: IR &lt;- M, PC &lt;- PC+1</li><li><strong>Decode Instructions</strong><br>After the instruction has been fetched, processor needs to identify which instruction has been fetched to invoke the correct execute cycle. During the decode cycle, processor identifies the execution cycle by the opcode of the instruction and passes down to the execute cycle to carry out the rest of the operation.</li><li><strong>Execute Instruction</strong><br>Execute instructions are the main types of instructions that carry out the major parts of the processing. The execute instructions can be classified according to the tasks they perform.</li></ul><p>You can get a clear understanding about the instructions and their functionalities in the <a href="https://github.com/ShechemKS/DownSampling_Processor/blob/master/Pocessor%20Design%20Report.pdf">project report</a>. </p><p>Finally, the state diagram of the complete process looked like this.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.32.41.png" class="kg-image" alt="FPGA Based Processor Implementation" loading="lazy" width="1766" height="610" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-00.32.41.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-00.32.41.png 1000w, http://localhost:2368/content/images/size/w1600/2021/12/Screenshot-2021-12-12-at-00.32.41.png 1600w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.32.41.png 1766w" sizes="(min-width: 720px) 720px"><figcaption>State Diagram of the Processor</figcaption></figure><h2 id="algorithm">Algorithm</h2><p>The algorithm for the image down-sampling process consists of two main sections which are the filtering of the image and the down-sampling itself. First, the filtering algorithm is applied to the whole image by selecting 3x1 and 1x3 pixel sets and then the down-sampling algorithm is applied. More details about each of these algorithms will be explained below.</p><h3 id="filtering-algorithm">Filtering Algorithm</h3><p>Filtering is done by using a gaussian kernel. Instead of using a 3x3, 2-D gaussian kernel, we opt kernel separability where 2 kernels that are 1x3 and 3x1 run through the image as shown below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.35.41.png" class="kg-image" alt="FPGA Based Processor Implementation" loading="lazy" width="1090" height="200" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-00.35.41.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-00.35.41.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.35.41.png 1090w" sizes="(min-width: 720px) 720px"><figcaption>Filter Kernels</figcaption></figure><p>The main reason for the above selection was to make the algorithm more simple and easy to understand. At first vertical filtering was carried out and horizontal filtering was carried out on resultant image after the vertical filtering. Finally after the two filtrations, pixel values of the image were saved in the main memory. The weighted average was calculated by multiplying the pixel value and kernel value and adding the three likewise values together and dividing it by the weighted average of the kernel, which is 4 in both. The below illustration may give you a better understanding.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.37.11.png" class="kg-image" alt="FPGA Based Processor Implementation" loading="lazy" width="1242" height="482" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-00.37.11.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-00.37.11.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.37.11.png 1242w" sizes="(min-width: 720px) 720px"><figcaption>Applying the filter</figcaption></figure><p>After applying the vertical kernel shown below to the three green pixels in the image, the centre pixel value (red one) will get updated.</p><h3 id="downsampling-algorithm">Downsampling Algorithm</h3><p>After the filtering algorithm is applied to the image, we then apply the down sampling algorithm to the filtered image. The concept of the down sampling algorithm is to select each alternative pixel value. It is also done in two steps by selecting an alternative pixel values horizontally and skipping an entire row vertically. The below illustration will give you a better idea about this process.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.39.35.png" class="kg-image" alt="FPGA Based Processor Implementation" loading="lazy" width="988" height="456" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-00.39.35.png 600w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.39.35.png 988w" sizes="(min-width: 720px) 720px"><figcaption>Pixel Selection</figcaption></figure><p>Which will give us a resultant image as follows.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.40.00.png" class="kg-image" alt="FPGA Based Processor Implementation" loading="lazy" width="514" height="258"><figcaption>Resultant downsampled image</figcaption></figure><p>After coding the above algorithms in RTL using many modules created along the way such as registers, ALU (Arithmatic and Logic Units), Controller etc., RTL view of the processor can be shown as follows.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.44.15.png" class="kg-image" alt="FPGA Based Processor Implementation" loading="lazy" width="1690" height="1128" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-00.44.15.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-00.44.15.png 1000w, http://localhost:2368/content/images/size/w1600/2021/12/Screenshot-2021-12-12-at-00.44.15.png 1600w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.44.15.png 1690w" sizes="(min-width: 720px) 720px"><figcaption>RTL view of the processor</figcaption></figure><h2 id="error-detection">Error Detection</h2><p>In order to check how good our algorithm, we need to compare it with a reference result. Hence, for this task we used an opencv down sampled image from python. With the resultant image we get from python, we compared our resultant image from the processor and calculated the error.</p><p>Here, we have shown two images along with their opencv down sampled version and our processor down sampled version and the error between two images. Note that second image consists of many textures.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.46.41.png" class="kg-image" alt="FPGA Based Processor Implementation" loading="lazy" width="1236" height="942" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-00.46.41.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-00.46.41.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.46.41.png 1236w" sizes="(min-width: 720px) 720px"><figcaption>Result comparison (Low textured)</figcaption></figure><p>Error with the opencv down sampled image gives us 5% value. The main reason for this is the values saved on the memory of the FPGA was not ideal. Also the algorithm used by opencv is way more advanced than the one we used for out algorithm. Also numerical errors like floating points can affect this as well.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.48.16.png" class="kg-image" alt="FPGA Based Processor Implementation" loading="lazy" width="1204" height="1014" srcset="http://localhost:2368/content/images/size/w600/2021/12/Screenshot-2021-12-12-at-00.48.16.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/Screenshot-2021-12-12-at-00.48.16.png 1000w, http://localhost:2368/content/images/2021/12/Screenshot-2021-12-12-at-00.48.16.png 1204w" sizes="(min-width: 720px) 720px"><figcaption>Result comparison (High textured)</figcaption></figure><p>So, as you can our processor is capable of downsample an image by keeping the error level in a very low value. The difference between our downsampled image using machine instruction and the OpenCV based downsampled image is negligible. </p><p>This project was a huge success and this can be considered as one of the most interesting projects done as an undergraduate. If you want to know more about the project you can visit this <a href="https://github.com/ShechemKS/DownSampling_Processor">link</a> and look at the implementation. </p><p><br><br><br> </p>]]></content:encoded></item><item><title><![CDATA[KORSAL: Key-point Detection based Online Real-Time Spatio-Temporal Action Localization]]></title><description><![CDATA[<p></p><p>Real-time and online action localization in a video is a critical yet highly challenging problem. Accurate action localization requires utilization of both temporal and spatial information. Recent attempts achieve this by using computationally intensive 3D CNN architectures or highly redundant two-stream architectures with optical flow, making them both unsuitable for</p>]]></description><link>http://localhost:2368/korsal/</link><guid isPermaLink="false">61b4aafdf714c603360260d3</guid><category><![CDATA[Research]]></category><dc:creator><![CDATA[Sakuna Jayasundara]]></dc:creator><pubDate>Sat, 11 Dec 2021 13:46:50 GMT</pubDate><media:content url="http://localhost:2368/content/images/2021/12/android-user-activity-recognition-still-walking-running-driving.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://localhost:2368/content/images/2021/12/android-user-activity-recognition-still-walking-running-driving.jpg" alt="KORSAL: Key-point Detection based Online Real-Time Spatio-Temporal Action Localization"><p></p><p>Real-time and online action localization in a video is a critical yet highly challenging problem. Accurate action localization requires utilization of both temporal and spatial information. Recent attempts achieve this by using computationally intensive 3D CNN architectures or highly redundant two-stream architectures with optical flow, making them both unsuitable for real-time, online applications.</p><p>To accomplish activity localization under highly challenging real-time constraints, we propose utilizing fast and efficient key-point based bounding box prediction to spatially localize actions. We then introduce a tube-linking algorithm that maintains the continuity of action tubes temporally in the presence of occlusions.<br>Further, we eliminate the need for a two-stream architecture by combining temporal and spatial information into a cascaded input to a single network, allowing the network to learn from both types of information. Temporal information is efficiently extracted using a structural similarity index map as opposed to computationally intensive optical flow. Despite the simplicity of our approach, our lightweight end-to-end architecture achieves state-of-the-art frame-mAP of <strong>74.7% </strong>on the challenging UCF101-24 dataset, demonstrating a performance gain of &#xA0;<strong>6.4% </strong>over the previous best online methods. We also achieve state-of-the-art video-mAP results compared to both online and offline methods. Moreover, our model achieves a frame rate of &#xA0;<strong>41.8</strong> FPS, which is a <strong>10.7%</strong> improvement over contemporary real-time methods.</p><p>Over method of activity recognition is superior to the current existing research, because of several reasons.</p><ul><li>We utilize key-point based detection architecture for the first time for the task of ST action localization, which reduces model complexity and inference time over traditional anchor-box based approaches.</li><li>We demonstrate that the explicit computation of OF is unnecessary, and that the SSIM index map obtains sufficient inter-frame temporal information.</li><li>We show that the highly redundant two-stream architecture is unnecessary by providing a single network with both spatial and temporal information, and allowing it to extract necessary information through discriminative learning.</li><li>We introduce an efficient tube-linking algorithm that extrapolates the tubes for a short period of time using past detections for real-time deployment.<br></li></ul><p>Our proposed architecture can be shown as follows.</p><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2021/12/NewArchitecture.png" class="kg-image" alt="KORSAL: Key-point Detection based Online Real-Time Spatio-Temporal Action Localization" loading="lazy" width="1899" height="627" srcset="http://localhost:2368/content/images/size/w600/2021/12/NewArchitecture.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/NewArchitecture.png 1000w, http://localhost:2368/content/images/size/w1600/2021/12/NewArchitecture.png 1600w, http://localhost:2368/content/images/2021/12/NewArchitecture.png 1899w" sizes="(min-width: 720px) 720px"></figure><p>To obtain temporal information using SSIM Index Map, we used the following architecture.</p><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2021/12/SSIM_Calc.png" class="kg-image" alt="KORSAL: Key-point Detection based Online Real-Time Spatio-Temporal Action Localization" loading="lazy" width="1899" height="704" srcset="http://localhost:2368/content/images/size/w600/2021/12/SSIM_Calc.png 600w, http://localhost:2368/content/images/size/w1000/2021/12/SSIM_Calc.png 1000w, http://localhost:2368/content/images/size/w1600/2021/12/SSIM_Calc.png 1600w, http://localhost:2368/content/images/2021/12/SSIM_Calc.png 1899w" sizes="(min-width: 720px) 720px"></figure><h2 id="conclusion">Conclusion</h2><p>In this paper, we proposed a method to solve the challenging problem of <strong>online</strong> and <strong>real-time </strong>spatio-temporal action localization by utilizing simple and efficient key-point based detection architectures. We further improved upon existing linking algorithms to maintain temporal continuity by extrapolating the future positions of action tubes to compensate for missed detections online. We showed that the pre-computation of OF to capture motion information affects real-time performance, and we integrated temporal and appearance feature extraction into a single network. We demonstrated that our approach is able to run faster and achieve better performance than state-of-the-art methods on the UCF101-24 and J-HMDB-21 datasets.</p><p>Further extensions of this work can explore an integrated tube-linking algorithm and faster feature extraction backbones. Moreover, temporally aware feature extraction can also be investigated.</p><p>If you want to know more details about the research, you can visit the following link and checkout our <a href="https://arxiv.org/pdf/2111.03319.pdf">research paper</a>. </p>]]></content:encoded></item><item><title><![CDATA[Google Cloud Platform 101]]></title><description><![CDATA[<h2 id="core-infrastructure-%E2%80%94-introduction">Core Infrastructure &#x2014; Introduction</h2><p></p><p>Google cloud platform (GCP) is a platform offered by Google with main 4 services run on the infrastructure that Google uses for some well known applications such as Youtube and Google Search.What are these 4 main kinds?<br></p><ul><li>Compute&#x200A;&#x2014;&#x200A;App Engine, Compute Engine,</li></ul>]]></description><link>http://localhost:2368/google-cloud-platform-101/</link><guid isPermaLink="false">61abc07f6e3de41686ad0e4a</guid><category><![CDATA[Articles]]></category><dc:creator><![CDATA[Sakuna Jayasundara]]></dc:creator><pubDate>Sat, 04 Dec 2021 19:39:55 GMT</pubDate><media:content url="http://localhost:2368/content/images/2021/12/1-VPTDOCbtNRQSOwWmw_zERw.png" medium="image"/><content:encoded><![CDATA[<h2 id="core-infrastructure-%E2%80%94-introduction">Core Infrastructure &#x2014; Introduction</h2><img src="http://localhost:2368/content/images/2021/12/1-VPTDOCbtNRQSOwWmw_zERw.png" alt="Google Cloud Platform 101"><p></p><p>Google cloud platform (GCP) is a platform offered by Google with main 4 services run on the infrastructure that Google uses for some well known applications such as Youtube and Google Search.What are these 4 main kinds?<br></p><ul><li>Compute&#x200A;&#x2014;&#x200A;App Engine, Compute Engine, Kubernetes Engine, Cloud Functions etc..</li><li>Storage&#x200A;&#x2014;&#x200A;Cloud Storage, Cloud Big Table, Cloud Datastore, Persistent Disk etc..</li><li>Big Data&#x200A;&#x2014;&#x200A;Big Query, Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Cloud Composer</li><li>Machine Learning&#x200A;&#x2014;&#x200A;Cloud, AutoML, Cloud TPU, Cloud Vision API, Cloud Natural Language, Cloud Speech to Text, Cloud Video Intelligance etc..</li></ul><p>In this article I&#x2019;m giving the basic idea about what GCP is and how is it organized.We will be discussing about aforementioned four kinds in more details in my future articles.So let&#x2019;s move forward.</p><h4 id="what-is-cloud-computing">What is Cloud Computing ?</h4><p>Today world is massively depending on &#x201C;Cloud Computing&#x201D;. So what&#x2019;s exactly is this Cloud Computing? Here is the definition.</p><blockquote><em>&#x201C; Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.&#x201D;&#x200A;&#x2014;&#x200A;</em> <strong>Peter M. Mell</strong> <strong>Timothy Grance</strong></blockquote><p>Seems pretty hard to understand. Isn&#x2019;t it? &#x1F609;</p><p>So the basic idea of cloud computing is a way of using I.T. with following important traits.</p><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2021/12/1-VPTDOCbtNRQSOwWmw_zERw-1.png" class="kg-image" alt="Google Cloud Platform 101" loading="lazy" width="641" height="361" srcset="http://localhost:2368/content/images/size/w600/2021/12/1-VPTDOCbtNRQSOwWmw_zERw-1.png 600w, http://localhost:2368/content/images/2021/12/1-VPTDOCbtNRQSOwWmw_zERw-1.png 641w"></figure><p>You can allocate resources as much as you want without any human intervention. Only you have to do is to use an interface, then you will get all the facilities such as storage and processing power. You can access your allocated resources any where in the world. May be your allocated resources are located in a different part of the world. </p><p>Elasticity is a very important facility. You can scale your resources as you want quickly and more importantly you pay for what you are using. If you are not using a resource anymore, you don&#x2019;t pay.</p><p>So that&#x2019;s it .. !! You know now the basic idea of cloud computing with GCP.</p><p>There are several GCP computing architectures. You can use any of them as per your requirement.</p><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2021/12/1-9hWmMCwMwKRqQpVKybgxZA.png" class="kg-image" alt="Google Cloud Platform 101" loading="lazy" width="646" height="365" srcset="http://localhost:2368/content/images/size/w600/2021/12/1-9hWmMCwMwKRqQpVKybgxZA.png 600w, http://localhost:2368/content/images/2021/12/1-9hWmMCwMwKRqQpVKybgxZA.png 646w"></figure><ul><li>IaaS (Infrastructure as a Service)&#x200A;&#x2014;&#x200A;Provides raw compute,storage and network facilities and you have to pay what you allocate.</li><li>PaaS (Platform as a Service)&#x200A;&#x2014;&#x200A;This binds the application code to libraries that allows you to access the resources what your application needs and you pay what you use.</li><li>Saas (Software as a Service)&#x200A;&#x2014;&#x200A;Many applications like Gmail,Google Drive and othre GSuit applications based on this architecture</li></ul><p>You do not need to worry about the resource provisioning at all !!</p><p>In network perspective, did you know the 40% of day-to-day internet traffic is handled by Google? Google network is designed to provide customers the highest possible throughput while providing the lowest possible latency. To provide such an amazing throughput and latency Google uses so called &#x201C;Edge Network system&#x201D; to cite content near the end users.</p><p>Let&#x2019;s see how the GCP is organized.</p><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2021/12/1-o5asfkzuXHACBYBm-b1GrQ.png" class="kg-image" alt="Google Cloud Platform 101" loading="lazy" width="647" height="368" srcset="http://localhost:2368/content/images/size/w600/2021/12/1-o5asfkzuXHACBYBm-b1GrQ.png 600w, http://localhost:2368/content/images/2021/12/1-o5asfkzuXHACBYBm-b1GrQ.png 647w"></figure><p>Zone is the deployment area of the GCP.When you want to run a Virtual Machine (VM) in GCP, that would be placed on a zone you defined. Region is created using several zones. Regions are located in several geographical locations with round trip network latency under 5ms !! If you need to establish an application with high availability you should establish your application in multiple zones.That allows you to provide the service continuously without any failure. Google provides multi region resource allocation facility. Which means your resources are located at least two geographically different locations which are 160 km apart. The most important thing is you can use resources located in multiple regions for the sake of virtualization.</p><p>Google always thinks about the environment too.Running many servers full of resources requires very high power consumption(approx. 2% of world electricity).For that google build their data centers near regenerative energy sources like sea and 100% carbon neutral since 2007. A great example provides by the data center located in Finland.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2021/12/1-Q2gALaR8Qkb-boBWCiHwBA.jpg" class="kg-image" alt="Google Cloud Platform 101" loading="lazy" width="500" height="333"><figcaption>Data center in Finland</figcaption></figure><p>Finally, We&#x2019;ve reached to the end of &#xA0;the introduction to GCP article. Before I end this, let&#x2019;s talk about the security of the GCP.</p><p>Since millions of users use GCP, security is essential. Google make their own custom servers with a high hardware security chip named &#x201C;Titan&#x201D;. These servers use cryptographic signatures to make sure the correct software is booted. There are several layers of security in these servers which can be accessed by a very few of google employees.Then how the servers communicate each other securely? For that google uses a cryptographic remote procedure called data-on-the network. GCP encrypts all our PC traffic while flowing. In the Google logging page you need more than your username and password.Google uses an intelligent system to challenge users for additional information based on several risk factors such as a device logged in your account in a suspicious time and in a different location. Google allows you to use two-factor authorization to sign in. Inside the google infrastructure the intelligence system warns you about any suspicious activity.Not only that google conducts Red team exercises and simulated attacks to test and improve the responsiveness of the system.Every account needs U2F key to avoid phishing attacks.So you have nothing to worry about your security anymore.Google handles all !!</p><p>So we have discussed about some basics and techniques that we should know when we step into the GCP world. Let&apos;s talk more about GCP later. Till then Good Bye !!</p>]]></content:encoded></item></channel></rss>